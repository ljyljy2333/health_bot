import functools

from dotenv import load_dotenv, find_dotenv
from langchain import hub
from langgraph.prebuilt import ToolNode


import os
from langchain_openai import AzureChatOpenAI

# å¯¼å…¥åŸºæœ¬æ¶ˆæ¯ç±»ã€ç”¨æˆ·æ¶ˆæ¯ç±»å’Œå·¥å…·æ¶ˆæ¯ç±»
from langchain_core.messages import (
    BaseMessage,
    HumanMessage,
    ToolMessage,
    AIMessage
)
# å¯¼å…¥èŠå¤©æç¤ºæ¨¡æ¿å’Œæ¶ˆæ¯å ä½ç¬¦
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
# å¯¼å…¥çŠ¶æ€å›¾ç›¸å…³çš„å¸¸é‡å’Œç±»
from langgraph.graph import END, StateGraph, START
# å¯¼å…¥æ“ä½œç¬¦å’Œç±»å‹æ³¨è§£
import operator
from typing import Annotated, Sequence, TypedDict, List
from langchain_community.tools.tavily_search import TavilySearchResults

import asyncio
from typing import Literal
# å¯¼å…¥è¿‡æ»¤å™¨

"""
æ‰§è¡ŒAgentçš„æ‰§è¡Œå™¨, è¾“å…¥ä¸ºå­—å…¸å½¢å¼çš„æ¶ˆæ¯ï¼Œè¾“å‡ºä¸ºå­—å…¸å½¢å¼çš„æ¶ˆæ¯ã€‚ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ è½½ç¯å¢ƒå˜é‡
2. åˆ›å»ºAzureChatOpenAIå®ä¾‹ï¼Œå¹¶ç»‘å®šTavilySearchResultså·¥å…·
3. åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿ï¼Œå¹¶ç»‘å®šåˆ°AzureChatOpenAIå®ä¾‹
4. å®šä¹‰AgentStateç±»å‹
5. å®šä¹‰AgentèŠ‚ç‚¹å‡½æ•°ï¼Œç”¨äºå¤„ç†æ¶ˆæ¯
6. å®šä¹‰è·¯ç”±å™¨ï¼Œç”¨äºé€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
7. å®šä¹‰çŠ¶æ€å›¾ï¼Œå¹¶ç¼–è¯‘
8. è°ƒç”¨çŠ¶æ€å›¾çš„ainvokeæ–¹æ³•ï¼Œæ‰§è¡ŒAgentçš„æ‰§è¡Œ
9. ä¿å­˜çŠ¶æ€å›¾çš„å›¾ç‰‡åˆ°æ–‡ä»¶
10. å¼‚æ­¥æ‰§è¡ŒçŠ¶æ€å›¾ï¼Œå¹¶æ‰“å°è¾“å‡ºç»“æœ
:param messages_dict: è¾“å…¥çš„æ¶ˆæ¯å­—å…¸
:return: è¾“å‡ºçš„æ¶ˆæ¯å­—å…¸

"""

_ = load_dotenv(find_dotenv(), verbose=True,override=True)
# åˆ›å»ºTavilySearchResultså·¥å…·ï¼Œè®¾ç½®æœ€å¤§ç»“æœæ•°ä¸º1
tools = [TavilySearchResults(max_results=2)]
# å®šä¹‰AgentStateç±»å‹
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]

class ResearcherAgent:
    def __init__(self):
        self.llm = AzureChatOpenAI(
            azure_deployment=os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"],
            api_version=os.environ["AZURE_OPENAI_API_VERSION"],
            temperature=0
        )

        self.tool_node = ToolNode(tools)  # è‡ªåŠ¨æ‰¾åˆ°è°ƒç”¨å“ªä¸ªå·¥å…·ï¼Œä¼ å…¥ç”Ÿæˆçš„å‚æ•°

        self.graph = self._build_graph()
        graph_png = self.graph.get_graph().draw_mermaid_png()
        with open("csv_searcher.png", "wb") as f:
            f.write(graph_png)

    # å®šä¹‰ä¸€ä¸ªå·¥å…·å‡½æ•°ï¼Œç”¨äºç¼©çŸ­å·¥å…·æ¶ˆæ¯çš„å†…å®¹
    def _shrink_tool_message(self,msg: ToolMessage) -> ToolMessage:
        return ToolMessage(
            tool_call_id=msg.tool_call_id,
            name=msg.name,
            content="tool result omitted, deleted internally"
        )

    # å®šä¹‰ä¸€ä¸ªå·¥å…·å‡½æ•°ï¼Œç”¨äºè¿‡æ›¿æ¢ ToolMessage ç±»å‹çš„æ¶ˆæ¯
    def _filter_state(self,state: AgentState):
        last_msg = state["messages"][-1]

        msgs = []
        for m in state["messages"][:-1]:
            if isinstance(m, ToolMessage):
                msgs.append(self._shrink_tool_message(m))
            else:
                msgs.append(m)

        return {
            "messages": msgs + [last_msg],
        }

    # --- èŠ‚ç‚¹å‡½æ•° ---
    #èŠ‚ç‚¹æ¨¡ç‰ˆ
    async def agent_node(self,state: AgentState,agent):
        # åˆ¤æ–­æ˜¯å¦è°ƒç”¨å·¥å…·
        new_state = self._filter_state(state)

        result = await agent.ainvoke({"messages": new_state["messages"]})  # messgeplaceholder

        result = AIMessage(**result.dict(exclude={"type", "name"}))
        return {
            "messages": [result],
        }
    # å®šä¹‰æœç´¢èŠ‚ç‚¹å‡½æ•°
    async def research_node(self, state: AgentState):

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system",
                 "You are a helpful assistant.You must refer to the result of toolcall. If there is no result of toolcall, call tools."),
                MessagesPlaceholder(variable_name="messages"),
            ]
        )

        agent = prompt | self.llm.bind_tools(tools)

        # ğŸ‘‡ ç›´æ¥è°ƒç”¨ agent_node
        return await self.agent_node(state, agent=agent)
    def router(self,state: AgentState) -> Literal["call_tool", "__end__"]:
        # è¿™æ˜¯è·¯ç”±å™¨
        messages = state["messages"]
        last_message = messages[-1]
        # æ£€æŸ¥ last_message æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨ï¼ˆtool callsï¼‰
        if last_message.tool_calls:
            return "call_tool"
        else:
            return "__end__"
    def _build_graph(self):
        workflow = StateGraph(AgentState)
        workflow.add_node("researcher", self.research_node)
        workflow.add_node("call_tool", self.tool_node)

        workflow.add_edge(START, "researcher")
        workflow.add_edge("call_tool", "researcher")

        workflow.add_conditional_edges(
            "researcher",
            self.router,
            {
                "call_tool": "call_tool",
                "__end__": END,
            }
        )

        return workflow.compile()

    def run(self, messages_dict: dict):
        return self.graph.ainvoke(messages_dict,
        {"recursion_limit": 50})
# # åˆ›å»ºAgentå®ä¾‹
# agent = ResearcherAgent()
#
# all_messages = await graph.ainvoke(
#     messages_dict,
#     {"recursion_limit": 50},
# )
#
# print(all_messages)
# print("ToolMessage:"+all_messages["messages"][-2].content)
# return all_messages
